{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjbZS3Fnm1Tj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_features, embedding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder network\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embedding_dim)\n",
        "        )\n",
        "        # Decoder network\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)  # Generate embedding\n",
        "        reconstructed = self.decoder(z)  # Reconstruct input\n",
        "        return z, reconstructed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "input_features = 3  # Replace with your feature count\n",
        "embedding_dim = 16  # Desired embedding size\n",
        "\n",
        "model = Autoencoder(input_features, embedding_dim)\n",
        "criterion = nn.MSELoss()  # Reconstruction loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Example medical dataset (replace with real data)\n",
        "data = torch.tensor([\n",
        "    [25, 120, 180],\n",
        "    [45, 140, 200],\n",
        "    [30, 110, 170],\n",
        "    [60, 150, 240]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "epochs = 700\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    embeddings, reconstructed = model(data)\n",
        "    loss = criterion(reconstructed, data)  # Reconstruction loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1zdv5Jmm7js",
        "outputId": "aa8d4c37-2034-4756-dbc9-0aebd1a0e7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/700], Loss: 1021.0466\n",
            "Epoch [20/700], Loss: 167.4249\n",
            "Epoch [30/700], Loss: 88.9870\n",
            "Epoch [40/700], Loss: 39.3844\n",
            "Epoch [50/700], Loss: 35.1403\n",
            "Epoch [60/700], Loss: 35.6892\n",
            "Epoch [70/700], Loss: 29.3556\n",
            "Epoch [80/700], Loss: 28.2129\n",
            "Epoch [90/700], Loss: 27.0331\n",
            "Epoch [100/700], Loss: 25.7000\n",
            "Epoch [110/700], Loss: 24.0020\n",
            "Epoch [120/700], Loss: 21.8400\n",
            "Epoch [130/700], Loss: 18.8214\n",
            "Epoch [140/700], Loss: 14.6392\n",
            "Epoch [150/700], Loss: 9.8757\n",
            "Epoch [160/700], Loss: 6.8912\n",
            "Epoch [170/700], Loss: 6.5983\n",
            "Epoch [180/700], Loss: 6.2493\n",
            "Epoch [190/700], Loss: 6.0610\n",
            "Epoch [200/700], Loss: 5.9362\n",
            "Epoch [210/700], Loss: 5.8190\n",
            "Epoch [220/700], Loss: 5.7111\n",
            "Epoch [230/700], Loss: 5.5980\n",
            "Epoch [240/700], Loss: 5.4821\n",
            "Epoch [250/700], Loss: 5.3606\n",
            "Epoch [260/700], Loss: 5.2331\n",
            "Epoch [270/700], Loss: 5.0984\n",
            "Epoch [280/700], Loss: 4.9558\n",
            "Epoch [290/700], Loss: 4.8044\n",
            "Epoch [300/700], Loss: 4.6434\n",
            "Epoch [310/700], Loss: 4.4723\n",
            "Epoch [320/700], Loss: 4.2958\n",
            "Epoch [330/700], Loss: 5.5514\n",
            "Epoch [340/700], Loss: 5.5616\n",
            "Epoch [350/700], Loss: 3.6888\n",
            "Epoch [360/700], Loss: 4.2313\n",
            "Epoch [370/700], Loss: 3.5611\n",
            "Epoch [380/700], Loss: 2.8571\n",
            "Epoch [390/700], Loss: 2.5503\n",
            "Epoch [400/700], Loss: 2.2332\n",
            "Epoch [410/700], Loss: 1.9030\n",
            "Epoch [420/700], Loss: 1.5939\n",
            "Epoch [430/700], Loss: 1.3036\n",
            "Epoch [440/700], Loss: 1.0353\n",
            "Epoch [450/700], Loss: 0.8034\n",
            "Epoch [460/700], Loss: 0.6236\n",
            "Epoch [470/700], Loss: 0.4582\n",
            "Epoch [480/700], Loss: 0.3370\n",
            "Epoch [490/700], Loss: 0.2477\n",
            "Epoch [500/700], Loss: 0.1712\n",
            "Epoch [510/700], Loss: 0.1210\n",
            "Epoch [520/700], Loss: 0.0856\n",
            "Epoch [530/700], Loss: 0.0620\n",
            "Epoch [540/700], Loss: 0.0446\n",
            "Epoch [550/700], Loss: 0.0300\n",
            "Epoch [560/700], Loss: 0.0195\n",
            "Epoch [570/700], Loss: 0.0129\n",
            "Epoch [580/700], Loss: 0.0126\n",
            "Epoch [590/700], Loss: 0.0078\n",
            "Epoch [600/700], Loss: 0.0145\n",
            "Epoch [610/700], Loss: 0.0060\n",
            "Epoch [620/700], Loss: 0.0047\n",
            "Epoch [630/700], Loss: 0.0037\n",
            "Epoch [640/700], Loss: 0.0032\n",
            "Epoch [650/700], Loss: 0.0029\n",
            "Epoch [660/700], Loss: 0.0025\n",
            "Epoch [670/700], Loss: 0.0023\n",
            "Epoch [680/700], Loss: 0.0020\n",
            "Epoch [690/700], Loss: 0.0018\n",
            "Epoch [700/700], Loss: 0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    embeddings, _ = model(data)\n",
        "print(\"Embeddings:\")\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv4OmHi3nSKY",
        "outputId": "bdaa47a1-e297-4f1f-f908-3afa22266956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings:\n",
            "tensor([[-125.2480,  156.6535,   -6.8873,   51.4001,   31.5073, -120.3581,\n",
            "          -84.5897,  -11.4361,   96.0744,  -89.5336,    4.5023,  196.4605,\n",
            "           11.7368,  -53.5631,  166.7162, -151.2204],\n",
            "        [-136.3131,  185.3941,  -13.6775,   56.0211,   34.4248, -133.7130,\n",
            "         -101.9095,    7.9001,  108.6089, -102.8195,    7.2231,  218.5744,\n",
            "           12.7295,  -64.6131,  196.7199, -168.8745],\n",
            "        [-117.9387,  150.1666,   -8.8668,   40.9546,   31.0175, -112.1260,\n",
            "          -82.4616,   -5.1451,   88.4239,  -83.8037,    5.9264,  183.0388,\n",
            "           15.5580,  -51.1031,  158.0094, -141.2779],\n",
            "        [-165.0250,  219.6606,  -19.0791,   41.5951,   46.2803, -155.0174,\n",
            "         -123.8403,   10.0417,  120.2823, -117.5182,   12.3695,  253.1760,\n",
            "           31.3709,  -74.5163,  227.3016, -196.4103]])\n"
          ]
        }
      ]
    }
  ]
}